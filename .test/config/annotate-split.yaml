# sample information sheet in TSV format (columns: sample, unit, fq1, fq2)
sample_list: config/samples.tsv

paths:
  # main base folder for results
  results: "results"
  # temporary path
  # set to $TMPDIR, /scratch or equivalent when running on HPC clusters
  temp: "temp"

# set the type of preprocessing to run
preprocessing:
  # run fastqc on (preprocessed) input?
  # if no preprocessing is done, fastqc will be run on the raw reads
  fastqc: False
  # trim reads with trimmomatic? (quality and adapter trimming)
  trimmomatic: False
  # trim reads with cutadapt? (runs instead of trimmomatic, no quality trimming)
  cutadapt: False
  # run fastuniq (removes duplicates from paired-end samples)
  fastuniq: False
  # map reads agains the phix genome and keep only reads that do not map concordantly
  phix_filter: False
  # run SortMeRNA to identify (and filter) rRNA sequences
  sortmerna: False

# parameters for trimmomatic
trimmomatic:
  # also trim adapters (in addition to quality trimming)?
  trim_adapters: True
  # settings specific to paired end reads
  pe:
    # adapter type to trim from paired end libraries with trimmomatic
    # ["NexteraPE-PE", "TruSeq2-PE", "TruSeq3-PE", "TruSeq3-PE-2"]
    adapter: "TruSeq3-PE-2"
    # parameters for trimming adapters on paired-end samples
    adapter_params: "2:30:15"
    # parameters for trimming prior to adapter removal on paired-end samples
    pre_adapter_params: ""
    # parameters for trimming after adapter removal on paired-end samples
    post_adapter_params: "LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:31"
  # settings specific to single-end reads
  se:
    # adapter type to trim from single end libraries with trimmomatic
    # ["TruSeq2-SE", "TruSeq3-SE"]
    adapter: "TruSeq3-SE"
    # parameters for trimming adapters on single-end samples
    se_adapter_params: "2:30:15"
    # parameters for trimming prior to adapter removal on single-end samples
    pre_adapter_params: ""
    # parameters for trimming after adapter removal on single-end samples
    post_adapter_params: "LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:31"

# parameters for cutadapt
cutadapt:
  # adapter sequence to trim with cutadapt
  # shown here is for Illumina TruSeq Universal Adapter.
  adapter_sequence: AGATCGGAAGAGCACACGTCTGAACTCCAGTCA
  # reverse adapter sequence to trim with cutadapt
  rev_adapter_sequence: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT
  # maximum allowed error rate as value between 0 and 1 (no. of errors divided by length of matching region)
  error_rate: 0.1 #Maximum allowed error rate as value between 0 and 1

# parameters for sortmerna
sortmerna:
  # sortmerna produces files with reads aligning to rRNA ("rRNA" extension)
  # and not aligning to rRNA ("non_rRNA") extension
  # which reads should be used for downstream analyses
  keep: "non_rRNA"
  # remove filtered reads (i.e. the reads NOT specified in "keep:")
  # if you are filtering out rRNA reads and don't plan to use them downstream
  # it may be worthwhile to set this to True since they can take up a lot of disk space
  remove_filtered: False
  # databases to use for rRNA identification
  dbs:
    - "rfam-5s-database-id98.fasta"
    - "rfam-5.8s-database-id98.fasta"
    - "silva-arc-16s-id95.fasta"
    - "silva-arc-23s-id98.fasta"
    - "silva-bac-16s-id90.fasta"
    - "silva-bac-23s-id98.fasta"
    - "silva-euk-18s-id95.fasta"
    - "silva-euk-28s-id98.fasta"
  # put both paired reads into rRNA bin (paired_in) or both reads in other bin (paired_out)
  paired_strategy: "paired_in"
  # extra settings for sortmerna
  extra_settings: "--num_alignments 1"

# remove duplicates from bam files?
remove_duplicates: True

assembly:
  # run Megahit assembler?
  megahit: False
  # Use Metaspades instead of Megahit for assembly?
  metaspades: True

megahit:
  # maximum threads for megahit
  threads: 20
  # keep intermediate contigs from Megahit?
  keep_intermediate: False
  # extra settings passed to Megahit
  extra_settings: "--min-contig-len 300 --prune-level 3"

metaspades:
  # maximum threads for metaspades
  threads: 20
  # keep intermediate contigs from Metaspades?
  keep_intermediate: False
  # keep corrected reads produced during Metaspades assembly?
  keep_corrected: True
  # extra settings passed to Metaspades
  extra_settings: "-k 21,31,41,51,61,71,81,91,101,111,121"

annotation:
  # split prodigal output into multiple files and run pfam_scan in parallel?
  # set below to number of files to split into
  splits: 10
  # run tRNAscan-SE?
  tRNAscan: False
  # run infernal for rRNA identification?
  infernal: False
  # run eggnog-mapper to infer KEGG orthologs, pathways and modules?
  eggnog: False
  # run PFAM-scan to infer protein families from PFAM?
  pfam: True
  # run Resistance gene identifier?
  rgi: False
  # run taxonomic annotation of assembled contigs (using tango + sourmash)?
  taxonomy: False

# params for taxonomic annotation of contigs/orfs
taxonomy:
  # minimum length of contigs to use for taxonomic annotation
  min_len: 300
  # parameters for tango search
  # use more permissive settings for search compared to assign in order to
  # modify assignments without having to rerun the search step
  search_params: "--evalue 0.01 --top 10"
  # parameters for tango assign
  assign_params: "--evalue 0.001 --top 5"
  # hash fraction to use for sourmash when computing signatures for contigs
  # this is evaluated as 1/<sourmash_fraction>
  sourmash_fraction: 100
  # kmer size to use when computing signatures
  sourmash_kmer_size: 31
  # sourmash database to use
  # see https://sourmash.readthedocs.io/en/latest/databases.html
  # and choose an LCA database link matching the sourmash_kmer_size
  sourmash_database_url: https://osf.io/ypsjq/download
  # ranks to report taxonomy for
  ranks:
    - "superkingdom"
    - "phylum"
    - "class"
    - "order"
    - "family"
    - "genus"
    - "species"
  # protein database to use for taxonomic assignments
  # choose between uniref50, uniref90, uniref100 and nr.
  taxdb: "uniref100"

binning:
  # minimum contig lengths to use for binning. binning will be run for every
  # length threshold with output under <results_path>/binning/{binner}/{contig_length}
  # typically shorter contig lengths will result in more a larger share of the
  # assembly being binned, at the cost of bin purity
  contig_lengths:
    - 1500
    # uncomment and/or add below to run binning at more lengths
    #- 2500
    #- 5000
  # run Metabat2 binner?
  metabat: False
  # run CONCOCT binner?
  concoct: False
  # run MaxBin2 binner?
  maxbin: False
  # maximum threads for binners
  threads: 20
  # run Checkm to assess quality of bins?
  checkm: False
  # run gtdbtk to classify bins phylogenetically?
  gtdbtk: False

# parameters for MaxBin2
maxbin:
  # maxbin2 uses marker gene identification on contigs to generate seed contigs
  # for binning. choose to use either markerset 40 (prokaryotes) or 107 (bacteria only)
  markerset: 40

# parameters for checkm
checkm:
  # run checkm taxonomy wf instead of lineage wf for bin QC?
  # setting this to True can be beneficial when you want a rough estimate of
  # completeness without having to run the resource heavy step of placing
  # bins into a reference tree
  taxonomy_wf: False
  # rank to use for checkm taxonomy wf
  rank: life
  # taxon to use for checkm taxonomy wf
  taxon: Prokaryote
  # use a reduced pplacer tree for checkm (uses less RAM)
  reduced_tree: False

# parameters for bowtie2
bowtie2:
  # maximum threads
  threads: 10
  # extra params to pass to bowtie2
  extra_settings: "--very-sensitive"

# what type of read-based classification to do
classification:
  # run kraken2 read classifier?
  kraken: True
  # run centrifuge to classify reads
  centrifuge: False
  # run metaphlan profiler?
  metaphlan: False

# parameters for kraken
kraken:
  # below are some options for creating the kraken database
  # the default is to use the prebuilt and lightweight "minikraken" database but
  # if you instead want to build the standard database you may do so, just keep
  # in mind that it will use a lot of resources during the build step

  # generate the standard kraken database?
  standard_db: False
  # download a prebuilt kraken2 database from the CCB servers
  # choose from "minikraken_8GB","16S_Greengenes","16S_RDP","16S_Silva"
  prebuilt: "minikraken_8GB"
  # if you already have access to a built kraken database you may specify the
  # database path here (path must contain hash.k2d, opts.k2d and taxo.k2d files
  custom: ""
  # should kraken2 run with reduced memory requirements?
  # setting reduce_memory to True makes kraken2 run in "--memory-mapping" mode
  # which avoids loading database into RAM and uses less memory
  reduce_memory: False

# centrifuge params
centrifuge:
  # the workflow uses prebuilt centrifuge databases that are downloaded from
  # CCB servers. choose from:
  # "p+h+v", "nt_2018_2_12", "nt_2018_3_3", "p_compressed+h+v" or "p_compressed_2018_4_15"
  # see http://ccb.jhu.edu/software/centrifuge/ for more info
  prebuilt: "p_compressed+h+v"
  # if you already have access to a built centrifuge database you may specify
  # the path to it here.
  custom: ""
  # minimum score for classifications by centrifuge.
  # because centrifuge doesn't have a filtering algorithm,
  # we use this min_score to filter results.
  min_score: 75
  # maximum number of assignments per read
  # by default this is set to 5 in centrifuge
  # set to 1 to implement LCA-classification as in Kraken
  max_assignments: 1

# params for metaphlan
metaphlan:
  # Version of the metaphlan database to use
  index: "mpa_v30_CHOCOPhlAn_201901"
  # What rank to summarize and plot a clustermap of metaphlan output
  plot_rank: "genus"

# other params
# this sets the number of reads to generate for example input files
# the files are generated based on the config/samples.tsv file and stored
# under examples/data
example_dataset_size: 100000
